{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "- Robert Shaw\n",
    "- CS109a Project: Data Driven March Madness\n",
    "\n",
    "In this file, we make predictions for years 2015-16 using the methods tuned on years 2001-2013 (see modeling files). We make both a single prediction bracket for an office pool type setting, as well as 10 brackets for ESPN bracket challenge. We compare our results to the ESPN field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import march_madness_classes as mmc\n",
    "import march_madness_games as mmg\n",
    "import march_madness_models as mmm\n",
    "import march_madness_train_and_tune as mmtt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression as LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataset\n",
    "teams = pd.read_csv(\"datasets/kaggle_data_2021/MTeams.csv\")\n",
    "seeds = pd.read_csv(\"datasets/kaggle_data_2021/MNCAATourneySeeds.csv\")\n",
    "slots = pd.read_csv(\"datasets/kaggle_data_2021/MNCAATourneySlots.csv\")\n",
    "games = pd.read_csv(\"datasets/kaggle_data_2021/MNCAATourneyCompactResults.csv\")\n",
    "#games_2016 = pd.read_csv(\"datasets/kaggle_data_2021/MNCAATourneyCompactResults2016.csv\")\n",
    "\n",
    "seeds_arr = mmg.filter_into_seasons(seeds)\n",
    "slots_arr = mmg.filter_into_seasons(slots)\n",
    "games_arr = mmg.filter_into_seasons(games)\n",
    "\n",
    "#games_arr.append(games_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract predictors, chosen from the variable selection notebook\n",
    "markov          = pd.read_csv(\"datasets/our_data/stationary\", index_col=0)\n",
    "rpi             = pd.read_csv(\"datasets/our_data/regular_season_rpi_matrix\", index_col=0)\n",
    "bad_losses      = pd.read_csv(\"datasets/our_data/bad_losses_matrix\", index_col=0)\n",
    "\n",
    "# seeds\n",
    "seed_matrix_df  = pd.read_csv(\"datasets/our_data/team_summary_data/seeds_matrix\", index_col=0)\n",
    "\n",
    "# get data into correct format\n",
    "predictor_names = [\"min_index_id\", \"max_index_id\", \"markov\",\"rpi\",\"bad_losses\"] \n",
    "\n",
    "# package the predictors into an array\n",
    "predictor_dfs = [markov, rpi, bad_losses] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 predictors, markov, rpi, and bad losses. See our data exploration/model selection for information on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reload' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-10e36feb8a1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmmg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reload' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(mmg)\n",
    "importlib.reload(mmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Setup Head to Head Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d133e5908fb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                              \u001b[0mgames_arr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                              \u001b[0mpredictor_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                              predictor_dfs)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# add to our array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/march_madness/march_madness_train_and_tune.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(window, test_yr, seeds_arr, slots_arr, tourney_arr, column_names, predictor_dfs, scoring_dif)\u001b[0m\n\u001b[1;32m     19\u001b[0m                                      \u001b[0mcolumn_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                      \u001b[0mpredictor_dfs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                      \u001b[0mscoring_dif\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring_dif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                                      )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/march_madness/march_madness_games.py\u001b[0m in \u001b[0;36mgenerate_multiple_years_of_games\u001b[0;34m(years, seed_list_arr, slot_list_arr, tourney_data_arr, predictors, predictor_dfs, scoring_dif)\u001b[0m\n\u001b[1;32m     35\u001b[0m                                                           \u001b[0mseed_list_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myear_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                                                           \u001b[0mslot_list_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myear_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                                                           \u001b[0mtourney_data_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myear_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                                                           \u001b[0mpredictors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                                           \u001b[0mpredictor_dfs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_test_arr = []\n",
    "\n",
    "window   = 5\n",
    "min_year = 2021\n",
    "max_year = 2022\n",
    "\n",
    "year_range = range(min_year, max_year)\n",
    "\n",
    "# generate our train test split for each year\n",
    "for year in year_range:\n",
    "    # do the split for the current year\n",
    "    train_test_tuple = mmtt.train_test_split(window, \n",
    "                                             year, \n",
    "                                             seeds_arr, \n",
    "                                             slots_arr, \n",
    "                                             games_arr, \n",
    "                                             predictor_names, \n",
    "                                             predictor_dfs)\n",
    "    \n",
    "    # add to our array\n",
    "    train_test_arr.append(train_test_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a \"windowing\" approach. The head to head model is trained on the 3 years prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# our cross validated value of c, from variable selection notebook\n",
    "c = 1\n",
    "variables = [\"markov\",\"rpi\",\"bad_losses\"] \n",
    "\n",
    "# models and scalers to be fit\n",
    "models  = []\n",
    "scalers = []\n",
    "\n",
    "for year in year_range:\n",
    "        # get train data\n",
    "        train_x = train_test_arr[year - min_year][0][variables]\n",
    "        train_y = train_test_arr[year - min_year][1].values[:, 0]\n",
    "\n",
    "        # get cross validation set\n",
    "        cross_x = train_test_arr[year - min_year][2][variables]\n",
    "        cross_y = train_test_arr[year - min_year][3].values[:, 0]\n",
    "\n",
    "        # scaling\n",
    "        scaler = StandardScaler().fit(train_x)\n",
    "        scaled_train_x = scaler.transform(train_x)\n",
    "        scaled_cross_x = scaler.transform(cross_x)\n",
    "\n",
    "        # init model\n",
    "        model = LogReg(C=c)\n",
    "\n",
    "        # fit model\n",
    "        model.fit(scaled_train_x, train_y)\n",
    "        \n",
    "        # append to our lis of models\n",
    "        models.append(model)\n",
    "        scalers.append(scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit head to head log reg model for predicting the outcomes of individual games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2) Predict A Single Bracket\n",
    "\n",
    "- a) Simulation, with Bias of .1 (Cross Validated in Other Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup simulator\n",
    "simulators = []\n",
    "\n",
    "i = 0\n",
    "# iterate years\n",
    "for year in year_range: \n",
    "    # get data needed\n",
    "    seeds_year = seeds_arr[year-1985] \n",
    "    slots_year = slots_arr[year-1985] \n",
    "    games_year = games_arr[year-1985]\n",
    "    \n",
    "    # setup head to head model, simulator with .07 bias\n",
    "    head_to_head_model_year = mmm.ModelPredictor(models[i], scalers[i], predictor_dfs, year, seeds_year, \n",
    "                                                 simulation=True,\n",
    "                                                 higher_seed_bias=True,\n",
    "                                                 higher_seed_bias_delta=.1)\n",
    "    \n",
    "    # setup simulator\n",
    "    simulators.append(mmc.Simulator(seeds_year, slots_year, head_to_head_model_year))\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n",
      "2016\n"
     ]
    }
   ],
   "source": [
    "# run simulations\n",
    "i = 0\n",
    "for year in year_range:\n",
    "    # run simulation\n",
    "    points = simulators[i].simulate_tournament(300) \n",
    "    \n",
    "    # predict bracket based on the simulation\n",
    "    bracket = simulators[i].predict_tournament()\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "    print(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- b) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison\n",
    "\n",
    "# run greedy and top seed tourneys\n",
    "year = min_year\n",
    "top_seed_tourneys = []\n",
    "actual_tourneys   = []\n",
    "\n",
    "# analyze results for all simulations\n",
    "i = 0\n",
    "for year in year_range:\n",
    "    # get data from our db\n",
    "    seeds_year = seeds_arr[year-1985] \n",
    "    slots_year = slots_arr[year-1985] \n",
    "    games_year = games_arr[year-1985]\n",
    "    \n",
    "    # get actual models\n",
    "    actual_model = mmm.ActualTournament(games_arr[year-1985])\n",
    "    actual_tourneys.append(mmc.Tournament(seeds_year, slots_year, actual_model, include_scoring_dif=False))\n",
    "    \n",
    "    # get top seed models\n",
    "    top_seed_model = mmm.BasicPredictor()\n",
    "    top_seed_tourneys.append(mmc.Tournament(seeds_year, slots_year, top_seed_model, include_scoring_dif=False))\n",
    "    \n",
    "    year = year + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Points  : 940\n",
      "\n",
      "Total Accuracy: 40 / 63 = 0.6349206349206349\n",
      "R1    Accuracy: 22 / 32 = 0.6875\n",
      "R2    Accuracy: 10 / 16 = 0.625\n",
      "R3    Accuracy: 5 / 8 = 0.625\n",
      "R4    Accuracy: 2 / 4 = 0.5\n",
      "R5    Accuracy: 1 / 2 = 0.5\n",
      "R6    Accuracy: 0 / 1 = 0.0\n",
      "Total Points  : 1320\n",
      "\n",
      "Total Accuracy: 43 / 63 = 0.6825396825396826\n",
      "R1    Accuracy: 22 / 32 = 0.6875\n",
      "R2    Accuracy: 11 / 16 = 0.6875\n",
      "R3    Accuracy: 6 / 8 = 0.75\n",
      "R4    Accuracy: 2 / 4 = 0.5\n",
      "R5    Accuracy: 1 / 2 = 0.5\n",
      "R6    Accuracy: 1 / 1 = 1.0\n"
     ]
    }
   ],
   "source": [
    "year = min_year\n",
    "\n",
    "simulator_scores = np.zeros(len(year_range))\n",
    "top_seed_scores = np.zeros(len(year_range))\n",
    "\n",
    "# analyze results for all simulations\n",
    "i = 0\n",
    "for simulator in simulators:\n",
    "    # get data from our db\n",
    "    seeds_year = seeds_arr[year-1985] \n",
    "    slots_year = slots_arr[year-1985] \n",
    "    games_year = games_arr[year-1985]\n",
    "    \n",
    "    \n",
    "    # score tournament \n",
    "    simulator_scores[i], x = simulator.score_tournament(actual_tourneys[i], print_res=True)\n",
    "    top_seed_scores[i],  x = top_seed_tourneys[i].score_tournament(actual_tourneys[i], print_res=False)\n",
    "    \n",
    "    year = year + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([890., 870.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_seed_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 940., 1320.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulator_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simulation method does better than just predicting the top seed, especially in 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3) Predict 10 Brackets\n",
    "- a) Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffers\n",
    "ensembles = []\n",
    "\n",
    "i = 0\n",
    "for year in year_range:\n",
    "    # get data from our db\n",
    "    seeds_year = seeds_arr[year-1985] \n",
    "    slots_year = slots_arr[year-1985] \n",
    "    games_year = games_arr[year-1985]\n",
    "    \n",
    "    # setup ensembles\n",
    "    ensembles.append(mmc.Ensemble(seeds_arr[year-1985], \n",
    "                                   slots_arr[year-1985], \n",
    "                                   models[i], \n",
    "                                   scalers[i],  \n",
    "                                   predictor_dfs, \n",
    "                                   year))\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffers\n",
    "ensemble_scores = np.zeros(len(year_range))\n",
    "ind_bracket_scores = np.zeros((len(year_range), 10))\n",
    "\n",
    "ensemble_dif_matrix = np.zeros((len(year_range), 10, 10))\n",
    "ensemble_avg_dif = np.zeros((len(year_range), 10))\n",
    "ensemble_dif_top_seed = np.zeros((len(year_range), 10))\n",
    "\n",
    "i = 0\n",
    "for year in year_range:\n",
    "    # get data from our db\n",
    "    seeds_year = seeds_arr[year-1985] \n",
    "    slots_year = slots_arr[year-1985] \n",
    "    games_year = games_arr[year-1985]\n",
    "    \n",
    "    # setup ensembles\n",
    "    ensemble_tournament = ensembles[i]\n",
    "    \n",
    "    # individual bracket scores\n",
    "    ind_bracket_scores[i, :] = ensemble_tournament.score_tournament(actual_tourneys[i])\n",
    "    \n",
    "    # ensemble score\n",
    "    ensemble_scores[i] = np.max(ind_bracket_scores[i, :])\n",
    "    \n",
    "    # difference between brackets\n",
    "    ensemble_dif_matrix[i, :, :] = ensemble_tournament.compute_dif_matrix(actual_tourneys[i]) \n",
    "    ensemble_avg_dif[i, :] = ensemble_tournament.avg_game_dif()\n",
    "    ensemble_dif_top_seed[i, :] =  ensemble_tournament.compute_dif_vect(actual_tourneys[i], top_seed_tourneys[i])\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1010. 1260.]\n",
      "[ 940. 1320.]\n",
      "[890. 870.]\n"
     ]
    }
   ],
   "source": [
    "print(ensemble_scores)\n",
    "print(simulator_scores)\n",
    "print(top_seed_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the ensemble gives us a big boost in 2016, showing that the diversity of brackets is helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4) Compare Our Brackets to the Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2016\n",
    "    - Ensemble: 1260 is the 99.2 percentile for 2016.\n",
    "    - Single Bracket: 1000 is the 95 percentile for 2016.\n",
    "\n",
    "- 2015\n",
    "    - Ensemble: 980 is the 76 percentile for 2015.\n",
    "    - Single Bracket: 960 is the 75 percentile for 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5) Save Our Predictions\n",
    "\n",
    "Send Predictions to CSVs for use on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_team_name(bracket, teams):\n",
    "    strong_seed_names = []\n",
    "    weak_seed_names   = []\n",
    "    prediction_names  = []\n",
    "    \n",
    "    for index, row in bracket.iterrows():\n",
    "        # extract ids\n",
    "        strong_seed_id = int(row[\"Strongseed Team\"])\n",
    "        weak_seed_id   = int(row[\"Weakseed Team\"])\n",
    "        prediction_id  = int(row[\"Prediction\"])\n",
    "        \n",
    "        # reverse lookup\n",
    "        strong_seed_team = teams[teams[\"Team_Id\"] == strong_seed_id][\"Team_Name\"].values[0]\n",
    "        weak_seed_team = teams[teams[\"Team_Id\"] == weak_seed_id][\"Team_Name\"].values[0]\n",
    "        prediction_team = teams[teams[\"Team_Id\"] == prediction_id][\"Team_Name\"].values[0]\n",
    "        \n",
    "        # add to array\n",
    "        strong_seed_names.append(strong_seed_team)\n",
    "        weak_seed_names.append(weak_seed_team)\n",
    "        prediction_names.append(prediction_team)\n",
    "        \n",
    "    bracket[\"Strongseed Team Name\"] = strong_seed_names\n",
    "    bracket[\"Weakseed Team Name\"] = weak_seed_names\n",
    "    bracket[\"Prediction Team Name\"] = prediction_names\n",
    "    \n",
    "    return bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions\n",
    "\n",
    "prediction_2015 = simulators[0].tournament_prediction.entire_bracket\n",
    "prediction_2015 = add_team_name(prediction_2015, teams)\n",
    "prediction_2015.to_csv(\"datasets/predictions/2015_single_bracket_prediction.csv\")\n",
    "\n",
    "prediction_2016 = simulators[1].tournament_prediction.entire_bracket\n",
    "prediction_2016 = add_team_name(prediction_2016, teams)\n",
    "prediction_2016.to_csv(\"datasets/predictions/2016_single_bracket_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for year in year_range:\n",
    "    ensemble = ensembles[i]\n",
    "    \n",
    "    # iterate\n",
    "    j = 0 \n",
    "    for tourney in ensemble.tourney_arr:\n",
    "        filepath = \"datasets/predictions/{}_ensemble_bracket_{}_prediction.csv\".format(year, j)\n",
    "    \n",
    "        # add team names\n",
    "        bracket = tourney.entire_bracket\n",
    "        bracket = add_team_name(bracket, teams)\n",
    "        bracket.to_csv(filepath)\n",
    "        \n",
    "        j = j + 1\n",
    "        \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for year in year_range:\n",
    "    actual_results = actual_tourneys[i].entire_bracket\n",
    "    \n",
    "    filepath = \"datasets/predictions/{}_actual_results\".format(year)\n",
    "    \n",
    "    # add team names\n",
    "    actual_results = add_team_name(actual_results, teams)\n",
    "    actual_results.to_csv(filepath)\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for year in year_range:\n",
    "    top_seed_results = top_seed_tourneys[i].entire_bracket\n",
    "    \n",
    "    filepath = \"datasets/predictions/{}_low_seed_prediction\".format(year)\n",
    "    \n",
    "    top_seed_results = add_team_name(top_seed_results, teams)\n",
    "    top_seed_results.to_csv(filepath)\n",
    "    \n",
    "    i = i + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
