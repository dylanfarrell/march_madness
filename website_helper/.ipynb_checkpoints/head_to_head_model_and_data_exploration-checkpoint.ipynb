{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Head to Head Model Website</h1>\n",
    "\n",
    "<h4> 1) Intro </h4>\n",
    "\n",
    "<p>In order to predict the NCAA tournament, first we need to create a head to head model, which given data for 2 teams, predicts the outcome of the game. Obviously, the markov chain stationary distribution (explained <b> <a href=\"link\"> Here </a> </b>) was one of the first predictors that we explored. However, given our knowledge of college baskeball, we also wanted to try some other predictors to see if a combination of them did a better job. We considered all of the following predictors:</p>\n",
    "\n",
    "<ul>\n",
    "<li>avg_points_against</li>\n",
    "<li>avg_points_for</li>\n",
    "<li>away_wins</li>\n",
    "<li>bad_losses (losses against not tournament teams)</li>\n",
    "<li>close_games (games decided in OT or by < 1 basket </li>\n",
    "<li>close_wins (wins in close games)</li>\n",
    "<li>close_wins_perc (% of wins in close games)</li>\n",
    "<li>consistency (std of scoring dif)</li>\n",
    "<li>def_eff (points per possession by opponents)</li>\n",
    "<li>dominance (avg of scoring dif)</li>\n",
    "<li>good_wins_matrix (wins vs tourney teams)</li>\n",
    "<li>luck (wins - win ration)</li>\n",
    "<li>stationary (markov stationary distributions)</li>\n",
    "<li>momentum (wins in the last 30 days)</li>\n",
    "<li>off_eff (points per possestions)</li>\n",
    "<li>past_results (tournament wins in the prior 2 years)</li>\n",
    "<li>points_against</li>\n",
    "<li>points_for</li>\n",
    "<li>rpi ((Win Percentage x 0.25) + (Opp Win Percentage x 0.50) + (Opp's Opp Win Percentage x 0.25))</li>\n",
    "<li>tempo (possestions/ game)</li>\n",
    "<li>tempo_opp (opp possessions / game)</li>\n",
    "<li>tough_wins (wins vs teams in the tournament on the road)</li>\n",
    "<li>weighted_wins (wins x 1/ seed)</li>\n",
    "<li>win_percentage (wins / games)</li>\n",
    "<li>win_percentage_vs_tourney_teams (wins vs tourney teams / games vs tourney teams)</li>\n",
    "<li>win_ratio (Expected Wins = (points for)^11 / ((points for ^ 11) + (points against) ^ 11)</li>\n",
    "</ul>\n",
    "\n",
    "<h4> 2) Data Exploration </h4>\n",
    "<p> We originally explored these variables by plotting them against post-season wins between 1985 and 2015. We see the some of the results below. The x-axis is the value of the statistic in the regular season and the y-axis is the number of wins in the post season:<p>\n",
    "\n",
    "<ul>\n",
    "<li><img src=\"img/pi_v_tournament_wins_lin_regression.png\"></li>\n",
    "<li><img src=\"img/rpi.png\"></li>\n",
    "<li><img src=\"img/bad_losses.png\"></li>\n",
    "<li><img src=\"img/win_pct_vs_tournament_teams.png\"></li>\n",
    "<li><img src=\"img/good_wins.png\"></li>\n",
    "<li><img src=\"img/consistency.png\"></li>\n",
    "<li><img src=\"img/dominance.png\"></li>\n",
    "<li><img src=\"img/away_wins.png\"></li>\n",
    "<li><img src=\"img/tough_wins.png\"></li>\n",
    "<li><img src=\"img/defense.png\"></li>\n",
    "<li><img src=\"img/offense.png\"></li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "<h4> 3) Model Selection </h4>\n",
    "\n",
    "<p>Obviously, many of these values are highly correlated. As such, in order not to overfit our head to head model, we performed variable selection. To do this, we created 8 logistic regression models for classifying the winners of games from 2006 to 2013. We used a \"windowing approach,\" where the model for 2006 is trained on 2003, 2004, and 2005, the model for 2007 is trained on 2004, 2005, and 2006 and so forth. This method allowed us to have a large number of observations for our training, but also prevented overfitting to the train data. Since each year we standardized the variables to have mean 0 and variance 1, the coefficients in the 8 logisitc regression models are all in the same units. As such, we could compare which variables were most important by looking at the average value over the 8 years and comparing the absolute values. See the results below. The x-axis is the average value of the coefficient over the 8 years. The y-axis is the variable.<p>\n",
    "\n",
    "<img src=\"img/coefficients_logistic.png\">\n",
    "\n",
    "<p> As we can see, markov, rpi and bad_losses are by far the most important predictors for our head to head models, with beta values of .84, .59, and -.41, respectively. To be clear on the interpretation : for every 1 standard deviation change in the difference between the stationary distribution of team 1 and team 2, the log odds increase by .84, and likewise for rpi and bad_losses.</p> \n",
    "\n",
    "\n",
    "<h4> Backwards Stepwise Selection</h4>\n",
    "<p>Since we did not have many predictors, we decided to perform backwards step-wise variable selection, removing 1 variable at a time (the one with the smallest absolute valued coefficient) and calculate the score (% correct on the test set). By doing this, we are essentially cross validating the optimal number of variables to include in our model. See the results below. The x-axis the number of predictors and the y-axis is the % correct.<p>\n",
    "\n",
    "<img src=\"img/model_selection.png\">\n",
    "\n",
    "<p>As we can see, having 3 variables in the logistic model (markov, rpi, and bad_losses) maximized our accuracy on the test set. This also outperformed running random forest with all of the variables in the model and running a logistic model with variables selected by running a lasso regression on predicting the scoring difference of games.  As such, we have cross validated the number of variables to include in the model, 3. Thus, we chose a logisitic model with 3 variables markov, bad_losses, and rpi.</p>\n",
    "\n",
    "<h4> Cross Validating the Regularization Parameter </h4>\n",
    "\n",
    "<p> As with all logistic regression models, we needed to cross validate the optimal value of the regularization parameter. We found that the value of C had essentially no effect on our test accuracy. We think that this is the case, becuase we manually performed variable selection using backwards stepwise variable selection, so our model was never overfit to begin with. As such, we will chose to just use C=1. See the results below. The x-axis is the value of C (inv of regularization strength) and the y-axis is the % correct.\n",
    "\n",
    "<img src=\"img/cross_validate_c.png\">\n",
    "\n",
    "<p>See the python notebooks for this process:</p>\n",
    "<ul>\n",
    "<li> <a href=\"code_html/data_exploration_basic_code.html\">Basic Data Exploraton </a></li>\n",
    "<li> <a href=\"code_html/data_exploration_markov_code.html\">Markov Data Exploraton </a></li>\n",
    "<li> <a href=\"code_html/data_exploration_adv_code.html\">Advanced Data Exploration</a></li>\n",
    "<li> <a href=\"code_html/modeling_code.html\">Modeling and Variable Selection</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
